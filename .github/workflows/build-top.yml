name: Build from LLM-Stats (merge into top-leaderboards.json)

on:
  schedule:
    - cron: "25 5 * * *"   # tous les jours à 05:25 UTC
  workflow_dispatch:

permissions:
  contents: write

jobs:
  llmstats:
    runs-on: ubuntu-latest
    env:
      # Mets ici l'URL publique de la page/table à parser (ou un CSV/JSON direct si tu en as un)
      LLM_STATS_URL: https://llm-stats.com/   # <- tu peux remplacer par l'URL exacte du tableau filtré
    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          python -m pip install pandas lxml html5lib requests

      - name: Write builder
        run: |
          cat > llmstats_to_top.py << 'PY'
          import os, re, json, math, requests
          import pandas as pd

          URL = os.environ.get("LLM_STATS_URL", "").strip()
          if not URL:
              raise SystemExit("LLM_STATS_URL manquant")

          def to_float(x):
              if x is None: return None
              if isinstance(x, (int, float)): return float(x)
              s = str(x).strip().replace(",", "")
              s = s.replace("$","")
              if s in ("", "-", "—", "–", "None", "null"): return None
              try:
                  return float(s)
              except:
                  # 88.4% -> 88.4
                  if s.endswith("%"):
                      try: return float(s[:-1])
                      except: return None
                  return None

          def parse_tokens_cell(x):
              """convertit 10,000K / 10M / 1 048 576 en entier de tokens"""
              if x is None: return None
              s = str(x).lower().replace("tokens","").replace(",","").replace(" ", "")
              if s in ("", "-", "—", "–"): return None
              # déjà un entier ?
              m = re.fullmatch(r"\d+", s)
              if m: return int(s)
              # 10m / 2.1m
              m = re.fullmatch(r"(\d+(?:\.\d+)?)m", s)
              if m: return int(float(m.group(1))*1_000_000)
              # 256k / 400k
              m = re.fullmatch(r"(\d+(?:\.\d+)?)k", s)
              if m: return int(float(m.group(1))*1_000)
              # 1_048_576
              m = re.fullmatch(r"(\d+)", re.sub(r"[_ ]","",s))
              if m: return int(m.group(1))
              return None

          def fmt_tokens(n):
              if n is None: return None
              if n >= 1_000_000: return f"{n/1_000_000:.1f}M tokens"
              if n >= 1_000:     return f"{int(n/1000)}K tokens"
              return f"{int(n)} tokens"

          # --- 1) Charger table depuis l'URL (HTML ou CSV/JSON si l'URL l'est)
          tables = []
          if URL.lower().endswith(".csv"):
              df = pd.read_csv(URL)
              tables = [df]
          elif URL.lower().endswith(".json"):
              # JSON → essaie DataFrame direct
              js = requests.get(URL, timeout=30).json()
              if isinstance(js, list):
                  tables = [pd.DataFrame(js)]
              elif isinstance(js, dict) and "data" in js:
                  tables = [pd.DataFrame(js["data"])]
              else:
                  raise SystemExit("JSON non reconnu")
          else:
              # HTML → pandas lit tous les tableaux
              tables = pd.read_html(URL)

          if not tables:
              raise SystemExit("Aucun tableau détecté sur l'URL")

          # --- 2) Choisir la table la plus “large”
          df = max(tables, key=lambda d: d.shape[1])

          # Normalise noms de colonnes (sans accents, minuscules)
          def norm(s): 
              return re.sub(r"[^a-z0-9]+"," ", str(s).lower()).strip()

          cols = {norm(c): c for c in df.columns}
          # Colonnes qu’on cherche
          col_org   = next((cols[k] for k in cols if k in ("organization","org","vendor","company","provider")), None)
          col_model = next((cols[k] for k in cols if k in ("model","name","model name")), None)
          col_gpqa  = next((cols[k] for k in cols if k in ("gpqa","gpqa score")), None)
          col_mmmu  = next((cols[k] for k in cols if k in ("mmmu","multimodal","vision")), None)
          col_mmlu  = next((cols[k] for k in cols if k in ("mmlu","mmlu score")), None)
          col_ctx   = next((cols[k] for k in cols if k in ("context","input context length","input tokens","context length")), None)
          col_inpm  = next((cols[k] for k in cols if k in ("input m","input m tokens","input price m","input m price","input m usd","input m tokens usd")), None)

          # --- 3) Extraire listes
          knowledge = []
          if col_gpqa and col_model:
              tmp = df[[col_model, col_gpqa]].dropna()
              for _, r in tmp.iterrows():
                  nm = str(r[col_model]).strip()
                  sc = to_float(r[col_gpqa])
                  if nm and sc is not None:
                      knowledge.append({"name": nm, "score": sc})
              knowledge.sort(key=lambda x: x["score"], reverse=True)
              knowledge = knowledge[:5]

          # Multimodal : MMMU prioritaire, sinon MMLU
          multimodal = []
          col_mm = col_mmmu or col_mmlu
          if col_mm and col_model:
              tmp = df[[col_model, col_mm]].dropna()
              for _, r in tmp.iterrows():
                  nm = str(r[col_model]).strip()
                  sc = to_float(r[col_mm])
                  if nm and sc is not None:
                      multimodal.append({"name": nm, "score": sc})
              multimodal.sort(key=lambda x: x["score"], reverse=True)
              multimodal = multimodal[:5]

          # Longest context
          longest_context = []
          if col_ctx and col_model:
              tmp = df[[col_model, col_ctx]].dropna()
              ctx_rows = []
              for _, r in tmp.iterrows():
                  nm = str(r[col_model]).strip()
                  v  = parse_tokens_cell(r[col_ctx])
                  if nm and v:
                      ctx_rows.append((nm, v))
              ctx_rows.sort(key=lambda x: x[1], reverse=True)
              longest_context = [{"name": nm, "value": fmt_tokens(v)} for nm, v in ctx_rows[:5]]

          # Cheapest provider (min input $/M par Organization)
          cheapest = []
          if col_inpm and col_org:
              tmp = df[[col_org, col_inpm]].dropna()
              best = {}
              for _, r in tmp.iterrows():
                  org = str(r[col_org]).strip()
                  c   = to_float(r[col_inpm])
                  if org and c is not None:
                      best[org] = c if org not in best else min(best[org], c)
              cheapest = [{"name": k, "value": f"${v:.2f} / 1M tokens"} for k, v in best.items()]
              cheapest.sort(key=lambda x: float(x["value"].split("$")[1].split("/")[0]))
              cheapest = cheapest[:5]

          # --- 4) Fusion avec un éventuel top-leaderboards.json existant
          existing = {}
          if os.path.isfile("top-leaderboards.json"):
              try:
                  existing = json.load(open("top-leaderboards.json","r",encoding="utf-8"))
              except Exception:
                  existing = {}

          data = {
              "code": existing.get("code", []),              # on conserve s’il existait
              "multimodal": multimodal or existing.get("multimodal", []),
              "knowledge": knowledge or existing.get("knowledge", []),
              "longest_context": longest_context or existing.get("longest_context", []),
              "cheapest": cheapest or existing.get("cheapest", []),
              "fastest": existing.get("fastest", [])         # pas disponible sur llm-stats → on conserve
          }

          with open("top-leaderboards.json","w",encoding="utf-8") as f:
              json.dump(data, f, indent=2, ensure_ascii=False)

          print("OK. Counts:", {k: len(v) for k,v in data.items()})
          PY

      - name: Commit & push if changed
        run: |
          echo "Changed files (before):"; git status --porcelain || true
          if ! git diff --quiet -- top-leaderboards.json; then
            git config user.name  "leaderboards-bot"
            git config user.email "actions@github.com"
            git add top-leaderboards.json
            git commit -m "chore: merge from llm-stats into consolidated JSON"
            git push
          else
            echo "No changes"
          fi
